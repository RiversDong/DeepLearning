#### RNN
循环神经网络，如下所示。    
如何做循环的：把当前词的输入传给下一个此。图中X1会拿到X0的信息，X2会拿到X1和X0的信息，即后面的会回去前面的所有信息。但是你看看X0不是直接给X2的，而是先给X1，X1把部分的X0传递给了X2。所以会出现什么情况，当我关注Xt的时候，如果t特别大，那么X0的信息就会微乎其微，**因此循环神经网络的特点是他无法做长序列**。当一句话达到50个字效果就会很差

![image](https://github.com/RiversDong/DeepLearning/assets/45725241/040e4e63-8139-4221-9666-eaa3221acf71)

#### LSTM
就有人解决了上述神经网络无法做长序列的问题，就是现在看到的LSTM模型，如下图所示是一个LSTM的模块。在RNN中其中的节点A只是做了一个很简单的线性变换，但是在LSTM中却做了记忆单元.例如在RNN中x0和X1的信息会传递给X2，但是LSTM会通过记忆们的形式给前面的变量记上权重，解决长程信息消失的问题。      
![image](https://github.com/RiversDong/DeepLearning/assets/45725241/69afd5f0-de23-4b8d-99fb-b613551eb59e)     

LSTM通过各种门，遗忘门、记忆门等选择性的记忆之前的信息，可以记忆更长的段落。可以达到200个词的信息

#### 自注意力和RNN的区别
RNN无法做长序列，注意力机制就可以解决RNN无法做长序列这个问题。从RNN的图中可以看出RNN其实是一个序列模型，只有在前后面的做完以后才能做当前的，当前的做完以后才能做下一个，因此RNN无法进行并行、另一个问题就是长序列信息消失。注意力机制可以避免这两个缺点。  
首先来看长序列以来问题是如何解决的。在自注意力机制中，句子中的某个词都会和其余的词进行相似度（注意力计算）的计算。既然做了这样的计算，再长的句子也可以获取到某个词和所有此的相似性（因为子注意力机制做了每个单词和句子中所有单词的注意力计算），因此从算法的原理上来讲自注意力机制解决了长序列依赖问题

![image](https://github.com/RiversDong/DeepLearning/assets/45725241/c110358c-238d-42a1-994c-48100e43b454)


其次，再来看看自注意力机制是如何解决并行问题的。先来看下面的图     
下面的图是每一个单词进行计算得到最终的新的特征表述Z1。同理可以用类似的方法从X2（初始的特征）————>Z2.....，而Z1和Z2的生成互不交叉，单独运行，也就是说所有的词可以并行的来生成对应词的Z，因此可以实现大规模的并行计算

![image](https://github.com/RiversDong/DeepLearning/assets/45725241/38b50adc-4594-4b7d-a654-12c4e5c1cc49)


此外自注意力机制不仅仅解决了RNN长序列信息消失和RNN的无法并行的问题，而且自注意力哦机制得到的新的词向量具有句法特征、和语义特征（词向量的表征更完善）。


