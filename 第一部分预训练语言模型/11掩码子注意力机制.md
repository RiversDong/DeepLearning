#### 上节回顾
* 注意力机制

  给以一张下面这个图的时候立刻联想到注意力机制在干什么事情 （Attention is all your need）。通过查询变量Q去查询V中有哪些东西是比较重要的，最终得到一个新的Z，Z就是对V的新的表征。K和V往往是相等的

![image](https://github.com/RiversDong/DeepLearning/assets/45725241/2de76292-77fc-439c-940a-ddbf388068b2)


* 自注意力机制

自注意力机制中的“自”其实表示的就是Q、K、V是同源的（即Q、K、V是来源于同一个X进行的线性变换），通过Q、K、V计算会得到下图所示的矩阵结果图：
I和I之间会有一个概率、I和have之间也有、I和a之间也有、I和dream之间也有，两两之间会有概率
![image](https://github.com/RiversDong/DeepLearning/assets/45725241/b78173a5-b5d9-4449-8fd1-ea65bd5218a2)

#### 掩码自注意力机制
为什么要做这样的改进呢？NLP中有个生成模型，生成单词，一个一个生成，问题就出在了一个一个生成上
当我们做生成任务的时候，我们也想对生成的单词做注意力计算，但是生成的句子是一个一个单词生成的    
就用下面的作为一个例子     
I have a dream
首先生成 I 接着       第一词注意力计算只有 I
I have  再接着是     这一次只有I 和 have进行自注意力计算
I have a     再接着才是     
I have a dream    
你会发现句子还没有生成结束的时候，不可能用到全部的词进行自注意力计算    
比如I 后面的还没生成，所以不可能用到后面的词进行自注意力计算    

所以需要一种新的模型去解决这个生成模型，所以掩码自注意力机制就产生了   例如下面的这个图   例如生成第一个单词的时候 后面的没有生成，所以遮盖住  因此生成I的时候 是I 和 I 进行的自注意力机制


![image](https://github.com/RiversDong/DeepLearning/assets/45725241/0c6103c6-34bc-4f46-8cbd-62b2d0424369)

因此自注意力机制和掩码自注意力机制的区别是：自注意力机制明确的指导这句话有多少个单词，并且一次性给足；    
而掩码自注意力机制是分批次给词，最后以此才能得到所有的信息     


未来到transformer的也会涉及到一些掩码自注意力机制




