#### 为什么需要位置编码呢？
先来看一下attention，他的优点是，1. 解决了长序列依赖问题；2，可以做到并行。但是他的确定啊是什么呢？开销变大了      
attention可以进行并行，每一个单词的计算可以同步进行。也就是说不存在顺序关系，词与词之间不存在顺序关系，因此注意力机制没有顺序关系。这是他的一个缺点     
换言之打乱一句话，这句话的每个词向量依然不会变。即无位置关系。     

因此需要位置编码解决注意力机制中的词向量没有位置关系的这个缺点。

#### 位置编码的原理是什么？
例如下面的图，原始的注意力机制是直接讲一个词传入到注意力机制中，但是现在传之前不仅仅是传递词的词向量，而且也把词的位置提取出来也做成一个词向量。也就是说位置编码加上词向量生成一个新的向量
然后把这个新的向量传递给注意力机制，也就是说这个新的词向量包含了位置信息也包好了词这个词向量的信息。
![image](https://github.com/RiversDong/DeepLearning/assets/45725241/b07b7a05-eaeb-47bc-b8b6-014099087938)     

位置编码具体由下面的公式给出，分别给出偶数位置的编码还有奇数位置的编码    
![image](https://github.com/RiversDong/DeepLearning/assets/45725241/85ff8bb3-5884-4a62-9124-7d4d3a2201db)    


另一种做法如下所示，词向量和位置的编码信息进行叠加形成新的词向量，生成的新的词向量包含了词的位置信息，新的词向量再输入到attention中去，这样经过注意力机制就得到了包含位置编码的词向量

![image](https://github.com/RiversDong/DeepLearning/assets/45725241/7ccf29a6-8f4c-4d62-8f11-8b991e818191)

#### 为什么进行位置编码以后会得到更加强大的词向量信息呢？请看下面的三角函数公式
![image](https://github.com/RiversDong/DeepLearning/assets/45725241/efa6107b-f644-477b-87bf-46655b6a2ca9)

#### 位置编码的补充



















