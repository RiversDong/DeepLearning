#### transformer的框架
引用的Seq2Seq的模型，由编码器和解码器组成    
1. 通过编码器对向量进行向量化
2. 把词向量输入到解码器，得到结果（生成单词）     

#### 编码器的基本框架
基本框架如下图所示    

一个编码器包含6个小的编码器，其中一个编码器包括两个子层：一个是self-attention、feed forward。    
每一个子层的传输过程中都会由一个残差网络+归一化。self-attention的输出会进行残差网络加上归一化，然后才传递给feed forward

![image](https://github.com/RiversDong/DeepLearning/assets/45725241/df531162-2824-488e-92a0-d00b3a77fc29)


下面是编码器的一个详细的图。   
首先是thinking会得到X1（可以通过one-hot编码可以通过word2vec等得到这个词向量），X1和位置编码进行叠加形成新的X1(给X1赋予位置属性，下图中黄色的X1)。然后黄色的X1输入到self-attention子层中进行自注意力机制。具体为X1和X1、X2拼接起来的一句话，X2和X1、X2拼接起来的一句话。得到Z1（X1与X1、X2拼接句子做了子注意力机制的词向量，他表征任然是thinking），也就是说Z1具有了位置特征、句法特征、语义特征----》接着Z1进行残差网络归一化（目的是避免梯度消失W2(w1x+b1)+b2,如果w1、W2、W3特别小，两者连乘会变的变得更小），所以要进行残差网络、归一化（归一化的一个作用是限制区间，避免梯度爆炸)，通过这个过程得到深粉色的Z1---》通过feed forward---》出来以后在经过残差网络和归一化得到最终的词向量。

```
forward做了一个什么事情呢？ feed forward前面的每一步都在做线性变换wx+b，线性变化的叠加永远都是线性变化，通过feed forward Relu(wx+b)，激活函数是让变化变成非线性变化，这样的空间变换可以拟合任何一种状态了。线性变换就是空间中的平移或者扩大或者缩小，无法变成其它的曲线，但是通过feed forward以后就的带了一个任意形状的曲线
```


![image](https://github.com/RiversDong/DeepLearning/assets/45725241/5129f669-cd0d-45af-a134-aae767d42af4)

#### 总结
transformer的编码器就是在做词向量，只不过这个词向量更加优秀








