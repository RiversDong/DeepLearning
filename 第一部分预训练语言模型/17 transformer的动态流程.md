#### 生成一个词
见下面的图片，以机器翻译为例，一个普通的词向量，加上位置编码组成一个带有位置编码的词向量，新的词向量
进入到encoder中进行 编码，编码后的词向量继续进行编码，编码的次数和encoder中的编码器的数量多少有关系 
最终得到encoder整个编码器的输出词向量    

encoders最终的输出作K和V矩阵传递给decoder，除此之外decoder还会接收已经生成的词去查询V中比较重要的信息

![image](https://github.com/RiversDong/DeepLearning/assets/45725241/e7afdbbf-f15e-478a-ba6f-13a9e47f5bd5)

```
bert其实是就是在transformer的基础上进行的改进，bert去除了transformer中的解码器，将编码器变成了12层，具有很强的词向量的生成能力
```

#### transformer框架    
其中存在两个为什么，这个要搞清楚   
1. 为什么decoder要用maske self attention
为了解决训练阶段和测试阶段的不匹配问题，
阶段解码器会有输入，这个输入是目标语句（翻译的结果）。通过已经生成的词其让编码器更好的生成词。
但是用普通的自编码给解码器输入目标语句的话会有问题。  什么问题呢？每一次都会把所有的信息告诉解码器

测试阶段：解码器也会有输出，但是此测试的时候不知道目标语句是什么，这个时候你每生成一个词就会多一个词放入到目标语句，每次生成的时候都是
已经生成的词（测试阶段只会把生成的词告诉解码器）    

为了匹配，解决测阶段和训练阶段的gap，必需用self-masked attention，我在训练阶段我就做一个mask，当你生成第一个词的时候什么也不提供---》当生成第一个词的时候只告诉第一个词的信息，。。。。。。 以此类推
```
机器翻译里面有源语句和目标语句，
```


2. 为什么编码器给予decoder的是K和V矩阵
Q来自于解码器，K=V来源于解码器
Q是查询变量，现在的Q是已经生成的词
K=V是源语句    
当我们生成这个词的时候通过已经生成的词和源语句做自注意力就是确定源语句中对接下来的词的生成更有作用。首先他就能找到当前生成的词

解决了以前的Seq2Seq的问题
以前的Seq2Seq的框架是用LSTM编码的（得到词向量），再用LSTM做解码器做生成
你会发现用这种方法去生成词，每一次生成词，都通过编码器的全部信息去生成，而且LSTM本身就有问题（长序列依赖问题）    
很多信息对于当前生成词没有意义（因此要做自注意力机制 ）





   








