## 1. 预训练
预训练有什么用处？深度学习的数据非常非常的大，大数据来支持。然而我们的很多项目没有大数据支持，数据量很少很少，预训练是为了解决这种问题而产生的。下面举一个例子，例如进行猫和狗的分类，数据量只有100张猫和狗的图片显而易见数据量太少了。但是如果有人做了如下的分类：鹅和鸭的分类，数据量很多：

>10万张鹅和鸭的图片分类，已经训练了一个模型A
是否可以用这个10万张鹅和鸭的分类模型（之前已经训练过）去做猫狗的分类（数据量较少的分类数据）问题呢？  经过实验验证发现之前训练模型可以用于当前数据量较少的类似的分类任务。

下面来说明原因。例如下面的人脸分类、汽车分类、大象分类、以及椅子分类，仔细观察无论是对上面的哪一个分类任务生成的图像在浅层的地方具有非常类似的情况，因此类似的分类任务浅层输出的结果类似。因此通过10万张鹅和鸭的模型，加上有100层的CNN。而我现在的任务是100张猫和狗的分类任务数据量很少，训练出100层的CNN很难实现。通过上面的论述可知鹅和鸭子的分类浅层网络的输出必然和猫狗分类的浅层网络的输出类似（因为是类似的分类任务），因此可以直接使用前者的浅层网络的参数来实验猫和狗的分类问题。
![image](https://github.com/RiversDong/DeepLearning/assets/45725241/0e1a4d60-0238-4a7c-9301-92667407a32b)。

具体怎么做呢？请看下面的图片。直接拿出A模型的浅层网络的参数（基于大数据训练的模型），通过任务B的数据进行训练再次训练反向出传播调整A模型浅层网络的参数。即A模型的浅层网络结构作为了B的浅层网络结构，而更深层的参数由当前的分类任务B进行训练，同时更新来自于A的浅层网络的参数。然后当网络比较复杂的时候，这样反向传播会浪费很多时间，因此可以冻结A的浅层网络参数。综上所述这种方法分为两种：
*冻结（Frozen）：浅层参数不变，即直接只是用A模型的浅层网络的参数和构架
*微调（Fine-Tuning）：浅层参数会跟着任务B的训练而改变，但是构架不会变
![image](https://github.com/RiversDong/DeepLearning/assets/45725241/6d08341d-369a-436c-ba8e-79b741a64f18)

### 1.1 什么是预训练
通过一个已经训练好的模型A（基于大数据训练的模型），去完成一个小数据量的任务B（使用了模型A的浅层参数）。那哪些模型A可以用来用于B呢？只有A和B分类任务相似的情况下才可以将A用于B的浅层网络。然后后面的bert和transformer一定程度上解决了这个问题。


### 1.2 总结
任务A和任务B两者及其相似。任务A已经训练除了一个模型A，使用模型A的浅层参数训练模型B得到模型B。训练B的时候可以微调A的浅层参数；或者冻结A的浅层参数（数据量较大，运行速度低的情况下使用微调的方式）
