### 回顾统计语言模型
####  是去完成下面的任务：用统计的方法完成一下两个和
* 比较
* 预测下一个词

### 神经网络语言模型
用神经网络的方法来解决统计语言模型来解决的问题，主要是针对第二个任务进行的，即预测下一个词。预测下一个单词是怎么做的呢？还是看下面的例子：  
“判断”，“一个”，“词”，"的"， “_____”   
为了解决统计语言模型计算量负责的缺陷，就提出了神经网络语言模型，如何做的呢？其实是用了两层感知机模型进行的求解，见下面的图    
![image](https://github.com/RiversDong/DeepLearning/assets/45725241/b2150cd2-4af3-4641-ae9e-78c39d7c50e4)    
这是一个很简答的双层感知机模型，
假设我们有四个单词w1，w2，w3，w4,进行下面的运算     
```
w1*Q=c1,
w2*Q=c2,
w3*Q=c3,
w4*Q=c4,
```
有人要好奇了w是什么，Q是什么呢？W是上述四个单词的独热编码，Q是什么呢？Q是一个随机矩阵(训练来的？)    
下面令    
```
C=[c1, c2, c3, c4]
```
接着进行感知机运算
```
softmax(U(tanh(WC+b1))+b2)
显而易见上述进行运算后会得到一个一维向量，表示各个词出现的概率
```  
>感知机模型是 激活函数(wx+b) 这样的形式，上述中的tanh和softmax都是激活函数。    

显而易见神经网络语言模型就是两层感知机，下面重点就来了，要引申出**词向量**，上述中的W是对词进行的编码，而Q是一个可以学习的随机矩阵。W*Q以后得到C，即代表了其中的每个单词。如果从很多则W会很大，但是我的Q只要选取合适，我就可以得到一个压缩后的值C。换言之随着Q的学习，C表示单词会越来越准确，也即是说C会越来越逼近这个词，然而神经网络语言模型的作者好像没有从中发现词向量的端倪（甚是可惜）。

### 词向量
可以说词向量是神经网络语言模型的副产品，假设上述的Q被训练好了。给我任何一个词，比如“判断”的独热编码， 比如编码成w1=[1,0,0,0]，则我可以用w1*Q得到c
w1*Q=c1    
c1就是判断这个词的词向量    
词向量就是用一个向量来表示一个单词，因此换言之独热编码也是一个词向量    
通过副产品Q做出来的词向量会更精确，而且可以缩小矩阵的维度，因此可以控制词向量的维度（Q选取的合适会缩小W的维度），来看下面的例子    
![image](https://github.com/RiversDong/DeepLearning/assets/45725241/7a589963-1c7e-4bd5-8f62-13960f49d4d2)
后面的C1和前面的W1相比维度变小了。后面用到的bert词向量的获取就是基于类似的原理进行的。显而易见如果我们得到了词向量，语言模型的第一个任务也被解决了（比较哪个概率更大，二分类任务）。


### 独热编码
将所有此看成一个词集合V，计算即不能认识这些词，使用编码的方式让计算机认识这些单词，假设有8个单词。    
![image](https://github.com/RiversDong/DeepLearning/assets/45725241/1aeef7b9-b011-4756-8423-6a24e7a1f253)

为了让计算机认识这些单词，要对这些词进行编码，**独热编码**就是一种编码方式，这个例子中有8个单词，则用8*8的0、1矩阵来编码这些词    
fruit-->00000001   
time-->00000010  
banana-->00000100  
显然独热编码有弊端
*  词向量多的话，编码的矩阵会很大
*  不能表示词于词之间的关联性，例如上面的例子中的banana和fruit之间的余弦值是0，便是正交，两个单词之间没啥关系

为了解决独热编码存在的问题有人提出了词向量的概念，词向量就是对词进行编码，让计算即可以认识，就是为了解决独热编码存在的这两个缺陷而进一步发展来了。




