### 回顾上节神经网络语言模型
#### 用神经网络语言模型去解决语言模型中的两个任务：   
* 两个句子比较谁出现的概率大,即分类问题
* 根据上文判断下一个词是什么
NNLM模型的主要目的是预测下一个词，而Word2Vec模型主要是为了得到词向量

#### 神经网络语言模型
其实就是两层感知机模型，神经网络的语言模型的**副产品**词向量。对于一个独热编码的词向量都可以通过可训练的Q矩阵得到一个新的词向量。新的词向量有两个好处：    
* 可以转化维度
* 相似词之间的词向量也有了关系

### World2Vec模型
他也是神经网络语言模型，其主要目的是得到词向量。下面的图是一个Word2Vec的示意图
![image](https://github.com/RiversDong/DeepLearning/assets/45725241/609ab0f5-0712-4cf4-b299-9076f0758db4)    
![image](https://github.com/RiversDong/DeepLearning/assets/45725241/75a92217-836c-470f-a9f3-c2c50026e9c9)

对比上面的两个图（将NNLM的图想右边旋转90独）。可以发现NNLM模型和Word2Vec模型基本一致（一模一样），不考虑细节，网络架构就是一样。    
word2Vec模型分为两种      
* CBOW
* Skip-gram

  #### CBOW
  给出一句话的==上下文==，然后得到这个词
  比如："我是最____的小明"
  假设____处应该填入的词为$w_t$,通过上文的$w_t-1$和下文的$w_t+1$来预测$w_t$

  #### Skip-gram
  给出一个词，得到这个词的上下文    
  例如给出了”帅“，上文是什么词，下文是什么词？
  即给定中间词，预测上下文的词是什么

  #### word2vec和神经网络语言模型的区别
* word2vec是为了得到词向量，而NNLM是为了预测下一个词。word2vec是为了得到词向量对于词的预测语句通顺即可，不许要很精确，意思相近的近义词也可以，而NNLM模型要精确。NNLM重点是预测下一个词，word2vec得到一个==Q矩阵==。Skip-gram和CBOW都是为了得到Q矩阵。可以这样去理解word2vec是一个老师（理解成给定的词）去训练处多个学生。  那为什么不理解成多个老师去训练一个学生呢？因为我们的目标是得到Q矩阵（即上图左边的部分）。但是skip-gram是多个老师训练一个学生。现在看了word2vec最重要的并不是输出了，而是输入，因为我们最终要的是Q矩阵。
* COBW一个老师告诉告诉多个学生Q怎么获得
*  Skip=gram是多个老师告诉一个学生，Q矩阵怎么变
*  NLM模型是双层感知机
*  体来看word2vec的结构是softmax(w1(xQ)+b1)

    因此你会看到word2vec和NNLM相比少了一个tanh，即一个激活函数。为什么word2vec不需要激活函数呢？这是因为word2vec目的是
    为了得到Q矩阵，而不是为了预测下一个词，因此不需要像NNLM那样需要一个激活函数.word2vec只需要能够反向传播就好了，这样更新Q矩阵，因此对于word2vec不需要一个额外的激活函数tanh来增加计算量。

> 激活函数的目的是为了让数据更方便、更容易的去拟合

#### word2vec的缺点是什么
缺点是Q矩阵的设计。现在来会议一下Q矩阵的设计（见下图），我们发现使用一个独热编码乘以一个Q矩阵的时候我们回到一个新的矩阵，例如这个地方的输出矩阵[10 12 19]，现在假00010代表的是apple，apple*Q=10 12 19,而这个东西也是代表的Apple。真是的Apple不贵，但是苹果手机确实很贵了。因此world2vec不能解决近义词的问题。
![image](https://github.com/RiversDong/DeepLearning/assets/45725241/8102626e-4f61-440a-9a68-d6c55089c12d)    

* 即词向量不能进行多义词

> 后面为了解决这个问题又提出了**ELMO模型**，这个模型在一定程度上关注了近义词，解决word2vec生词的词向量不能解决多义词的问题





    

