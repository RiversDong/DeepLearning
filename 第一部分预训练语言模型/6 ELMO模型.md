#### word2vec有缺点
他不能解决近义词的问题，为了解决这个问题有人提出了ELMO模型，比较好的解决了近义词的区分问题    
word2vec模型主要用来作词向量    
* CBOW
* Skip-gram

#### ELMO模型
ELMO模型见下面的图片，他是用来专门作词向量的（通过预训练的手段），解决word2vec不能解决近义词的问题，例如bank这个词有河岸和银行的银行的意思，解决以此多意的问题，如何解决多义词的问题呢？下面来看看具体是如何解决的。
![image](https://github.com/RiversDong/DeepLearning/assets/45725241/a1294bce-7f83-4b11-93d4-fd5bde51fe04)


* ELMO专门用来做词向量（通过预训练的手段），基于上下文的embedding
* ELMO不只是训练一个Q矩阵，还可以训练Q矩阵的同时，还可以把这个词的上下文的信息融入到Q矩阵中
* 来解释以下上面的这个图：左边的LSTM获得E2的上文信息。E1的信息传递给一个LSTM模型，然后当前的LSTM模型获得E1的信息，而后当前的LSTM写道的信息又传递给了下一个LSTM模型，而下一个LSTM也获得了E2的信息（所以左边的获取了E2的上文信息）。
* 然后再来看左边的部分看E3一直到En的词向量会一直传递给E2，所有E2对应的这个LSTM会获取下文的信息。
* 后面最后生成的T1、T2、T3则是通过这个预训练生成的新的一个词向量，你看T1的信息来自右边的部分也有来自左边的部分，因此T中含有了上文的信息也含有了下文的信息。
* 图中E1 E2 E3.。。。 等等这些词向量的生成的前一步是一个one-hot编码的矩阵，乘以了一个Q矩阵获取的。

综上所示T可以包含三个当面的特征，这三层信息组后做一个汇总形成最终的特征**T**
* 语义特征
* 语法特征
* 单词特征
  
![image](https://github.com/RiversDong/DeepLearning/assets/45725241/b3c7a3cb-f439-4be5-a54b-9a460bae9384)

#### 现在再来看word2vec和ELMO的区别的区别
word2vec中WQ直接得到一个词向量，但是在ELMO模型中是先WQ得到一个E，然后再传入到双向双层的LSTM模型中，分别获得上文信息和下文信息，最后汇总成T向量（包含语法特征、语意特征、句法特征）。

可以这样理解，ELMO输出的T1相当于T1=E1+E2+E3,约等于E1。T1中包含E1和E3相当于包含上下文信息。
进一步延申，可以做成三层的LSTM，也可以做成四层的LSTM

![image](https://github.com/RiversDong/DeepLearning/assets/45725241/dd101344-47c3-461d-aa10-b64fa719fdf4)

#### 怎么来解决多义词问题呢
比如E2是一个多义词，表示真实的苹果，这个水果在上下文中E1和E3肯定和真实的这个水果有关系。所以就知道E2其实就是真实的苹果，而不是我们买的苹果手机

#### LSTM的缺点
LSTM无法并行，长期依赖消失。数据量变大了无法并行头疼的很。  后面就有提出了一个可以做并行的和具有有关的框架模型称之为Attention，即注意力机制，这个框架来解决LSTM存在问题。左右后面就有人将ELMO模型中的LSTM模型或称了Attention模型（注意力机制），所以下一个笔记就来看看Attention这个框架。




