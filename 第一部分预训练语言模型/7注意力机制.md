#### 注意力机制（attention）
你会注意什么？大数据、人工智能，包包含重要的数据也包含非重要的数据。
对于重要的数据我们要使用，对于不重要的数据我们不不太想使用。
但是对于数据中什么数据是重要的数据，什么数据是非重要的数据不是我们个人能决定
但是对于一个模型而言，很难决定什么重要什么不重要，由此注意力机制诞生了，有人发现了如何去做注意力机制（如何在深度学习的模型上做注意力）

请看下面的图片。这张图和注意力有关，图中红色的是科学家们发现如果给你一张这个图你的重点会聚焦在红色区域。而红色区域的特点是脸。对于一段文字我们会看开头的部分。
* 人--》看脸

* 段落--》看口头

等等

![image](https://github.com/RiversDong/DeepLearning/assets/45725241/d9ba3e1c-cb1d-4759-99dc-0bc7f14356a8)

这些共色区域可能包含更重要的信息

#### 注意力机制进一步总结
我们会把我们的焦点聚焦在比较重要的事务上。

#### 如何做注意力机制
当我们看这张图的时候是我们在看，有一个我这个对象，还有这张图。我看作查询（Q，query），这张图看作被查询对象（V）

我看这图第一眼，我就会去判断哪些东西对我而言更重要（例如图片中有文字的部分），哪些东西对于我而言又更不重要（例如图片中白色的背景部分）。

* 模仿人的过程，去计算Q（query）和V（被查询的对象）里面的重要度。重要度计算其实就是相似度计算，对于Q而言V里面哪些东西重要,理解成Q和V里面的哪些东西更相似，或者说Q和V里面的哪些东西更接近。对于一个Q和K（k=k1，k2.。。。kn）而言，通过点乘计算Q和K里面的每一个事物的相似度，这样我们会有Q和K1的关系，Q和K2的关系等等（即Q和K中的任何事务之间的相似度），得到了n个相似值，假设相似值为s1, s2, s3.。。。
* 做一层sotfmax，sotfmax(s1, s2, s3.。。。)，就可以得到概率a1,a1.。。。，就可以哪个对于Q而言就重要了得到了对谁重要以后再进行一个汇总，相当于找到这些重要的事情以后，Q查询对象变得没有意义了。我们最终还是要拿到这张图片的，只不过现在的这张图片他多了一些信息，多了于我（Q）而言更重要的和更不重要的信息再里面。
* 原先的V=[v1,v2.v3...]，使用上述得到的(a1,a2,a3...)得到新的V‘ V'=[a1*v1, a2*v2.....]就包含了哪些重要和哪些不重要的信息了，用V'代替V。，一般K=V（在transfer里面是这样的）。Ｋ不等于V是否可以，可以，但是K和V要有联系，只有这样才能QK点成才能指导V中哪些重要，哪些不重要

![image](https://github.com/RiversDong/DeepLearning/assets/45725241/4851b76b-8a1d-4a9e-8e87-e947c6086fc8)

