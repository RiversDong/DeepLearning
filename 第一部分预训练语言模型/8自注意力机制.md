#### 上节回顾
当我们看一张图片的时候，我们会倾向于关注一些重点，比如下面的图，当我们第一眼看这个图的时候，我们首先会将眼光放到更重要的信息上

![image](https://github.com/RiversDong/DeepLearning/assets/45725241/a27507bd-99bf-415b-a2ff-bc06f65f7da1)

下图是attention的一些具体的流程
* Q个K相乘计算相似度，做一个scale（未来做softmax的时候避免出现极端情况）
* 然乎做softmax得到概率，即Q和K中每一个向量的相似情况
* 最后和V相乘得到最终的数值描述 新的向量表示了K和V，这种表示暗含了Q的信息（于Q而言K里面重要的信息），相当于对K进行了精简，跳出关于了K中的关键点。
  
![image](https://github.com/RiversDong/DeepLearning/assets/45725241/14dacf8c-7a3d-4e90-9431-b13cabf69b3a)

#### 自注意力机制（self-attantion）
Self-attention的关键带你在于不仅仅是K=V，而且K约等于V约等于Q来源于同一个X，这三者是同源的，因此叫子注意力机制。通过X找到X中的关键点，并不是K=V=Q，而是通过参数 $W_Q,W_K,W_V$ , 看下面的图：

![image](https://github.com/RiversDong/DeepLearning/assets/45725241/2c276d79-91da-4e17-a967-1725f539aa83)

* 给定一个X1，通过将X1分别和 $W_K,W_V,W_Q$ 相乘得到了新的q1, k1 和v1(这里也可以看出Q、K、V来自同一个X)

接下来的步骤和注意力机制一模一样。

![image](https://github.com/RiversDong/DeepLearning/assets/45725241/0528692e-5e4a-4d49-91fc-216767a17a06)

![image](https://github.com/RiversDong/DeepLearning/assets/45725241/3ba0f2ed-f80a-448f-91ef-f4e901c71b0d)

![image](https://github.com/RiversDong/DeepLearning/assets/45725241/bf9c9faa-aa94-4182-95df-11606937ba57)

$Z_1$表示的就是新的向量表示  

* 用图中的例子来看看自注意力机制是进行的
  
对于thinking初始为词向量x1
现在通过 tingking machines这句话去查询这句话里面的每一个单词和thinking之间的相似度    
新的Z1依然是thinking词向量的表示，只不过这个词向量的表示蕴含thinking machines这句话对于thinking而言哪个更重要的信息
同理machines这个单词也进行了类似的操作。


下面的图中its会和每一个单词进行自注意力计算，然后会发现通过子注意力机制得到的词向量its和law匹配程度很高，也就是说its会包含一定的law和application的信息在里面，当然其它词的信息也会包含在its生成的词向量里面

如果不做自注意力机制，its就是单纯的its，没有任何的附加信息，现在指导做自注意力机制的重要性了

![image](https://github.com/RiversDong/DeepLearning/assets/45725241/c5d24709-53d1-453d-bf1c-5231f4817f99)


#### 自注意力矩阵
首先X分别乘以$W_Q,W_K,W_V$得到Q、K、V，也就是说对于一组X我们通过三个参数的线性变换得到Q、K、V，因此Q、K、V是同源的都来源于一个X

![image](https://github.com/RiversDong/DeepLearning/assets/45725241/bfe7d530-bef1-4ad6-b95a-c1285d9ae159)

Q和K之间进行相似度的计算，进行sotfmax进行相似度的计算，最后乘以V得到Z，Z就是对于X而言新的词向量，Z会包含句子和句子之间内部的联系，Z中的每一个单词都会包含句子中的哪些单词对这个词重要的信息。例如我们之间提到的its和law以及application之间联系的例子

![image](https://github.com/RiversDong/DeepLearning/assets/45725241/2aa41b5c-6594-4a13-95eb-1d8354b1181b)    

自注意力机制特别的狭隘，属于注意力机制的，但是注意力机制包括自注意力机制，这是因为自注意力机制包含了Q、Ｋ、V同源

对一个词向量，做的空间上的线性变换，乘上了参数矩阵，依然代表X，是X的另外一种形式表达了X

　自己注意机制不仅规定了Q、K、V同源，而且也规定了怎么做。

 #### 交叉子注意力机制
 如果规定Q、K、V不相等就变成了交叉自注意力机制
在交叉自注意力机制中Q和V不同源，K和V同源

